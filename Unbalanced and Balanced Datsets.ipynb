{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5181d467",
   "metadata": {},
   "source": [
    "# Unbalanced and Balanced Datsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22c97a",
   "metadata": {},
   "source": [
    "eg. Fraud detection: Such transactions are rare, chanes are high that model is trained on an imbalanced datasets with less fraud transactions and a lot more non fraud transactions. Model might get high accuracy on training, however gves non- fraud answer everytime\n",
    "\n",
    "Size of positive and negative samples is the same means data is balanced whereas a huge variation in the sizes of the different class data suggests an imbalanced dataset.\n",
    "\n",
    "## Techniques to Convert Imbalanced Dataset into Balanced Dataset\n",
    "\n",
    "\n",
    "There are two main strategies for handling the class imbalance problem: data-level and algorithm-level techniques. Data-level techniques involve modifying the dataset to balance the classes, while algorithm-level techniques modify the learning algorithm to handle the imbalance.\n",
    "\n",
    "- Data-level techniques include undersampling, oversampling, and hybrid approaches. Undersampling involves randomly removing instances from the majority class to balance the dataset. Oversampling involves replicating instances from the minority class to balance the dataset. Hybrid approaches combine both undersampling and oversampling techniques.\n",
    "\n",
    "\n",
    "- Algorithm-level techniques include cost-sensitive learning, threshold-moving, and ensemble learning. Cost-sensitive learning involves assigning different misclassification costs to different classes. Threshold-moving involves adjusting the decision threshold to favor the minority class. Ensemble learning involves combining multiple classifiers to improve the overall performance\n",
    "\n",
    "### 1. Use the right evaluation metrics:\n",
    "\n",
    "    Evaluation metrics can be applied such as:\n",
    "\n",
    "- Confusion Matrix: a table showing correct predictions and types of incorrect predictions.\n",
    "- Precision: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n",
    "- Recall: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n",
    "- F1-Score: the weighted average of precision and recall.\n",
    "\n",
    "### 2. Over sampling/ Up sampling:\n",
    "\n",
    "When the quantity of data is insufficient, the oversampling method tries to balance by incrementing the size of rare samples. **Over-sampling increases the number of minority class members in the training set.**\n",
    "\n",
    "    Advantages:\n",
    "        - No information from the original training set is lost\n",
    "\n",
    "    Disadvantage:\n",
    "        - Overfitting\n",
    "        \n",
    "Some of the more widely used and implemented oversampling methods include:\n",
    "\n",
    "- Random Oversampling: randomly duplicating examples from the minority class\n",
    "\n",
    "\n",
    "- Synthetic Minority Oversampling Technique (SMOTE): works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample as a point along that line.\n",
    "\n",
    "\n",
    "- Borderline-SMOTE: Borderline-SMOTE involves selecting those instances of the minority class that are misclassified, such as with a k-nearest neighbor classification model, and only generating synthetic samples that are “difficult” to classify.\n",
    "\n",
    "\n",
    "- Borderline Oversampling with SVM: Borderline Oversampling is an extension to SMOTE that fits an SVM to the dataset and uses the decision boundary as defined by the support vectors as the basis for generating synthetic examples, again based on the idea that the decision boundary is the area where more minority examples are required.\n",
    "\n",
    "\n",
    "- Adaptive Synthetic Sampling (ADASYN): ADASYN) is another extension to SMOTE that generates synthetic samples inversely proportional to the density of the examples in the minority class. \n",
    "\n",
    "\n",
    "### 3. Under sampling/ Down sampling\n",
    "\n",
    "    Balances the imbalance dataset by reducing the size of the class which is in abundance. Methods for down sampling in classification:\n",
    "\n",
    "- The **cluster centroid methods** replace the cluster of samples by the cluster centroid of a K-means algorithm\n",
    "\n",
    "- **Tomek link method** removes unwanted overlap between classes until all minimally distanced nearest neighbors are of the same class. A Tomek Link refers to a pair of examples in the training dataset that are both nearest neighbors (have the minimum distance in feature space) and belong to different classes. Tomek Links are often misclassified examples found along the class boundary and the examples in the majority class are deleted.\n",
    "\n",
    "- The **Condensed Nearest Neighbors rule, or CNN** for short, was designed for reducing the memory required for the k-nearest neighbors algorithm. It works by enumerating the examples in the dataset and adding them to the store only if they cannot be classified correctly by the current contents of the store, and can be applied to reduce the number of examples in the majority class after all examples in the minority class have been added to the store.\n",
    "\n",
    "    Advantages:\n",
    "        - Run-time can be improved by decreasing the amount of training dataset.\n",
    "        - Helps in solving the memory problems\n",
    "\n",
    "    Disadvantage:\n",
    "        - Losing some critical information\n",
    "        \n",
    "### 4. Feature Selection:\n",
    "\n",
    "Performs the function of intelligent subsampling and potentially helps reduce the imbalance problem.\n",
    "\n",
    "Methods for feature selection on both classes:\n",
    "- One-sided metric such as correlation coefficient (CC) and odds ratios (OR) \n",
    "- two-sided metric evaluation such as information gain (IG) and chi-square (CHI)\n",
    "\n",
    "Based on the scores, we then identify the significant features from each class and take the union of these features to obtain the final set of features. \n",
    "\n",
    "Then, we use this data to classify the problem.\n",
    "\n",
    "### 5. Cost Sensitive Learning:\n",
    "\n",
    "The Cost-Sensitive Learning (CSL) takes the misclassification costs into consideration by minimising the total cost. \n",
    "\n",
    "### 6. Ensemble methods:\n",
    "\n",
    "The ensemble technique is combined the result or performance of several classiﬁers to improve the performance of single classiﬁer, thereby improving generalisability.eg. \n",
    "\n",
    "\n",
    "#### Bagging\n",
    "- Bagging-style methods aim to reduce the variance of the base classifiers by generating multiple bootstrap samples from the original dataset.\n",
    "\n",
    "\n",
    "- In the context of class imbalance learning, bagging can be used to create multiple models that are trained on different subsets of the minority class, which can help to improve the overall performance of the classifier.\n",
    "\n",
    "\n",
    "- One popular variant of bagging for class imbalance learning is SMOTEBagging, which combines the SMOTE oversampling technique with bagging to create multiple models that are trained on balanced subsets of the data.\n",
    "\n",
    "#### Boosting\n",
    "- Boosting-based methods focus on improving the accuracy of the base classifiers by iteratively reweighting the training examples. Combining multiple weak classifiers to create a strong classifier\n",
    "\n",
    "\n",
    "- In the context of class imbalance learning, boosting can be used to give more weight to misclassified instances of the minority class, which can help to improve the overall performance of the classifier.\n",
    "\n",
    "\n",
    "- One popular variant of boosting for class imbalance learning is RUSBoost, which combines random undersampling with boosting to create multiple models that are trained on balanced subsets of the data.\n",
    "\n",
    "Advantages:\n",
    "- Stable model with better predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e2f72",
   "metadata": {},
   "source": [
    "#### References:\n",
    "1. https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5\n",
    "2. https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/\n",
    "https://thecontentfarm.net/ensemble-techniques-for-handling-class-imbalance/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
