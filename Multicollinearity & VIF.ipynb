{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba90809",
   "metadata": {},
   "source": [
    "# Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112d0bc",
   "metadata": {},
   "source": [
    "Multicollinearity is a statistical phenomenon that occurs when two or more independent variables in a regression model are highly correlated with each other. This means that one independent variable can be predicted from another in a regression model.\n",
    "\n",
    "Eg. Eating chips and watching television are highly correlated in the case of a person if the person starts eating chips when they watch television and this makes him happy\n",
    "\n",
    "#####  Why is it problem?\n",
    "\n",
    "- It becomes difficult to determine the individual effects of each independent variable on the dependent variable accurately. That can be a problem when it comes to interpretability, though it doesnt directly impact accuracy.\n",
    "\n",
    "\n",
    "- Multicollinearity can lead to unstable and unreliable coefficient estimates in regression analysis, making it harder to interpret the results and draw meaningful conclusions from the model.\n",
    "\n",
    "    For example, letâ€™s assume that in the following linear equation:\n",
    "\n",
    "    Y = W0+W1*X1+W2*X2\n",
    "\n",
    "    Coefficient W1 is the increase in Y for a unit increase in X1 while keeping X2 constant. But since X1 and X2 are highly correlated, changes in X1 would also cause changes in X2, and we would not be able to see their individual effect on Y.\n",
    "\n",
    "##### How to identify and address Multicollinearity?\n",
    "\n",
    "When two variables have a correlation coefficient of either +1.0 or -1.0, they are considered perfectly collinear.\n",
    "\n",
    "1. Detecting Multicollinearity Using a Variance Inflation Factor (VIF)\n",
    "\n",
    "\n",
    "2. Correlation Matrix for independet variables\n",
    "\n",
    "\n",
    "3. Regularization and Feature Selection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7beb9f",
   "metadata": {},
   "source": [
    "\n",
    "### Variance Inflation Factor\n",
    "\n",
    "A variance inflation factor (VIF) is a measure of the amount of multicollinearity in regression analysis. It is predicted by taking a variable and regressing it against every other variable. \n",
    "\n",
    "or\n",
    "\n",
    "VIF score of an independent variable represents how well the variable is explained by other independent variables.\n",
    "\n",
    "$R^2$ value is determined to find out how well an independent variable is described by the other independent variables.\n",
    "\n",
    "**A high value of $R^2$ means that the variable is highly correlated with the other variables. Closer is $R^2$ to 1,  the higher the value of VIF and the higher the multicollinearity with the particular independent variable.**\n",
    "\n",
    "$$\n",
    "VIF_j = \\frac{1}{1 - R_j^2}\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "$R_j^2$ is the $R^2$ obtained by regressing the j-th predictor variable $X_j$ against all other predictor variables.\n",
    "\n",
    "\n",
    "\n",
    "- VIF starts at 1 and has no upper limit\n",
    "\n",
    "- VIF = 1, no correlation between the independent variable and the other variables\n",
    "\n",
    "- VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\n",
    "\n",
    "\n",
    "Although correlation matrix and scatter plots can also be used to find multicollinearity, their findings only show the bivariate relationship between the independent variables. VIF is preferred as it can show the correlation of a variable with a group of other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    \n",
    "    # Pass the column values and index of a column\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ad71e",
   "metadata": {},
   "source": [
    "## Fixing Multicollinearity:\n",
    "\n",
    "1. Dropping correlated features:\n",
    "\n",
    "X = df.drop(['Age','Years of Service'],axis=1)\n",
    "\n",
    "Dropping variables should be an iterative process starting with the variable having the largest VIF value because other variables highly capture its trend. If you do this, you will notice that VIF values for other variables would have reduced\n",
    "\n",
    "2. Next, combine the correlated variables into one and drop the others. This will reduce the multicollinearity.\n",
    "\n",
    "df2 = df.copy()\n",
    "df2['Age_at_joining'] = df.apply(lambda x: x['Age'] - x['Years of service'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be5b4b",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "1. https://www.analyticsvidhya.com/blog/2020/03/what-is-multicollinearity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7d0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
